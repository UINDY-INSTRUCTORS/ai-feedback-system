#!/usr/bin/env python3
"""
Generic Quarto/Markdown report parser.
Extracts content, structure, and a detailed map of figures for AI analysis.
Also extracts notebook cell outputs (tables, text, markdown, latex).
"""

import json
import yaml
import re
import sys
from pathlib import Path
from html_to_markdown import convert_notebook_output_to_markdown

def parse_quarto(file_path: str) -> dict:
    """
    Parse a Quarto (.qmd) document, find all figures (manual and generated),
    and map generated figures back to the embed shortcodes that created them.
    Also extracts all notebook cell outputs for comprehensive analysis.
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except FileNotFoundError:
        print(f"ERROR: Report file not found: {file_path}", file=sys.stderr)
        sys.exit(1)

    body = _get_body_content(content)

    figures_list = _extract_figures(body, Path(file_path).stem)
    structure = _extract_structure(body)
    notebook_outputs = _extract_notebook_outputs(body)
    stats = _calculate_stats(body, len(figures_list))
    supplementary_status = _check_supplementary_files()

    return {
        'content': body,
        'metadata': _get_yaml_metadata(content),
        'structure': structure,
        'figures': {
            'count': len(figures_list),
            'details': figures_list
        },
        'notebook_outputs': notebook_outputs,
        'stats': stats,
        'supplementary': supplementary_status
    }

def _get_yaml_metadata(full_content: str) -> dict:
    """Extracts YAML frontmatter from content."""
    yaml_match = re.match(r'^---\s*\n(.*?)\n---\s*\n', full_content, re.DOTALL)
    if yaml_match:
        try:
            return yaml.safe_load(yaml_match.group(1))
        except yaml.YAMLError as e:
            print(f"WARNING: Failed to parse YAML frontmatter: {e}")
    return {}

def _get_body_content(full_content: str) -> str:
    """Extracts the body (non-YAML) content."""
    if full_content.startswith('\ufeff'):
        full_content = full_content[1:]
    
    yaml_match = re.match(r'^---\s*\n(.*?)\n---\s*\n', full_content, re.DOTALL)
    body_start = yaml_match.end() if yaml_match else 0
    return full_content[body_start:]

def _extract_figures(body: str, report_stem: str) -> list:
    """
    Extracts all figures using a manual parser for markdown links
    and a regex for embeds. This is to bypass a subtle regex bug.
    """
    figures = []
    
    # 1. Manual parser for `![caption](path)`
    i = 0
    while i < len(body):
        i = body.find('![', i)
        if i == -1: break

        caption_start = i + 2
        caption_end = body.find(']', caption_start)
        if caption_end == -1:
            i += 2
            continue

        path_start = body.find('(', caption_end)
        if path_start != caption_end + 1:
            i = caption_end
            continue

        path_end = body.find(')', path_start)
        if path_end == -1:
            i = path_start
            continue

        caption = body[caption_start:caption_end]
        path = body[path_start + 1:path_end]
        
        figures.append({
            'path': path,
            'caption': caption.strip(),
            'source': f"markdown:{path}",
            'line': body.count('\n', 0, i) + 1
        })
        
        i = path_end

    # 2. Find embed shortcodes and map generated images
    embed_shortcodes = set(re.findall(r'\{\{<\s*embed\s+(.*?)\s*>\}\}', body))
    generated_images = _find_quarto_generated_images(report_stem)

    for img_path, img_caption in generated_images.items():
        if any(f['path'] == img_path for f in figures): continue

        matched_to_embed = False
        notebook_name_from_file = Path(img_path).name.split('-')[0].lower()
        
        for shortcode in embed_shortcodes:
            notebook_name_from_code = Path(shortcode.split('#')[0]).stem.lower()
            if notebook_name_from_file == notebook_name_from_code:
                figures.append({
                    'path': img_path, 'caption': img_caption,
                    'source': shortcode, 'line': -1
                })
                matched_to_embed = True
                break
        
        if not matched_to_embed:
            figures.append({
                'path': img_path, 'caption': img_caption,
                'source': 'generated:unmapped', 'line': -1
            })
            
    return figures

def _find_quarto_generated_images(report_stem: str) -> dict:
    """Finds images generated by Quarto and returns a dict of {path: caption}."""
    images = {}

    # Build list of possible figure directories
    # Quarto can generate figures in:
    # - {report_stem}_files/figure-*/ (if report was named something.qmd)
    # - index_files/figure-*/ (if report is index.qmd)
    # These can be at the root OR inside output/ directory
    base_names = set([f"{report_stem}_files", "index_files"])  # Use set to avoid duplicates
    search_base_dirs = [Path("."), Path("output")]

    print("   Scanning for Quarto-generated images...")
    print(f"      Working directory: {Path.cwd()}")
    for base_dir in search_base_dirs:
        print(f"      Checking base_dir: {base_dir}")
        if not base_dir.exists():
            print(f"        → Does not exist, skipping")
            continue
        for base_name in base_names:
            parent_dir = base_dir / base_name
            print(f"        Checking parent_dir: {parent_dir} (is_dir={parent_dir.is_dir()})")
            if parent_dir.is_dir():
                fig_dirs = list(parent_dir.glob('figure-*'))
                print(f"          Found {len(fig_dirs)} figure-* dirs")
                for fig_dir in fig_dirs:
                    if fig_dir.is_dir():
                        png_files = list(fig_dir.glob('*.png'))
                        print(f"            Found {len(png_files)} PNG files in {fig_dir.name}")
                        for image_path in png_files:
                            caption = f"Generated image from notebook cell: {image_path.name}"
                            images[str(image_path)] = caption
                            print(f"      Found generated image: {image_path}")
    return images

def _extract_structure(body: str) -> list:
    headings = re.findall(r'^(#{1,6})\s+(.+)$', body, re.MULTILINE)
    return [{'level': len(h[0]), 'text': h[1].strip()} for h in headings]

def _calculate_stats(body: str, figure_count: int) -> dict:
    text_only = re.sub(r'```.*?```|\{\{<.*?\}\}', '', body, flags=re.DOTALL)
    text_only = re.sub(r'\$\$.*?\$\$|\$[^$]+\$', '', text_only, flags=re.DOTALL)
    words = len(re.findall(r'\w+', text_only))

    return {
        'word_count': words,
        'code_blocks': len(re.findall(r'```\{python\}.*?```', body, re.DOTALL)),
        'equations': len(re.findall(r'\$\$.*?\$\$|\$[^$]+\$', body, re.DOTALL)),
        'figures': figure_count,
        'sections': len(_extract_structure(body))
    }

def _check_supplementary_files() -> dict:
    try:
        with open('.github/config.yml', 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
    except Exception:
        return {}

    supplementary_files = config.get('supplementary_files', [])
    status = {}
    for pattern in supplementary_files:
        matches = list(Path('.').glob(pattern))
        status[pattern] = {
            'exists': len(matches) > 0,
            'count': len(matches),
            'files': [str(p) for p in matches[:5]]
        }
    return status

def _extract_notebook_outputs(body: str) -> list:
    """
    Find all {{< embed ... >}} shortcodes and extract their cell outputs.
    Returns a list of dicts with embed info and extracted outputs.
    """
    embeds = re.findall(r'\{\{<\s*embed\s+(.*?)\s*>\}\}', body)
    notebook_outputs = []

    print("   Extracting notebook cell outputs...")

    for embed in embeds:
        # Parse embed shortcode (format: "notebook.ipynb#cell-id param1 param2")
        parts = embed.split()
        notebook_ref = parts[0]  # e.g., "notebook.ipynb#cell-id"

        ref_parts = notebook_ref.split('#')
        notebook_path = ref_parts[0].strip()
        cell_id = ref_parts[1] if len(ref_parts) > 1 else None

        # Try to find the notebook file - prefer OUTPUT notebooks (they have rendered results)
        nb_path = None
        nb_stem = Path(notebook_path).stem

        # First try output directory (has actual outputs)
        output_paths = [
            Path(f"output/{nb_stem}.out.ipynb"),
            Path(f"output/{notebook_path}"),
        ]

        for path in output_paths:
            if path.exists():
                nb_path = path
                break

        # Fall back to source notebook
        if not nb_path:
            source_paths = [
                Path(notebook_path),
                Path(f"./{notebook_path}"),
                Path(f"../{notebook_path}"),
            ]
            for path in source_paths:
                if path.exists():
                    nb_path = path
                    break

        if nb_path and nb_path.exists():
            try:
                outputs = _extract_cell_outputs_from_notebook(str(nb_path), cell_id)
                if outputs:
                    # Convert outputs to markdown format
                    converted = convert_notebook_output_to_markdown(outputs)

                    notebook_outputs.append({
                        'embed': embed,
                        'notebook': str(nb_path),
                        'cell_id': cell_id,
                        'outputs': converted
                    })
                    print(f"      Extracted outputs from {nb_path}#{cell_id or 'all'}")
                else:
                    print(f"      No outputs found in {nb_path}#{cell_id or 'all'}")
            except Exception as e:
                print(f"      WARNING: Failed to extract from {nb_path}: {e}", file=sys.stderr)
        else:
            print(f"      WARNING: Notebook not found: {notebook_path}", file=sys.stderr)

    return notebook_outputs

def _extract_cell_outputs_from_notebook(notebook_path: str, cell_id: str = None) -> dict:
    """
    Extract outputs from a Jupyter notebook cell.

    Args:
        notebook_path: Path to .ipynb file
        cell_id: Specific cell ID to extract (or None for all cells)

    Returns:
        Dict with keys: 'text', 'html', 'markdown', 'latex', 'images'
    """
    try:
        with open(notebook_path, 'r', encoding='utf-8') as f:
            nb = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"      ERROR reading notebook {notebook_path}: {e}", file=sys.stderr)
        return {}

    outputs_data = {'text': [], 'html': [], 'markdown': [], 'latex': [], 'images': []}

    for cell in nb.get('cells', []):
        # If cell_id specified, only process that cell
        if cell_id:
            # Match both the cell ID and common Quarto label formats
            cell_matches = (
                cell.get('id') == cell_id or
                cell.get('metadata', {}).get('label') == cell_id or
                cell.get('metadata', {}).get('tags', []) and cell_id in cell.get('metadata', {}).get('tags', [])
            )
            if not cell_matches:
                continue

        # Extract outputs from this cell
        for output in cell.get('outputs', []):
            output_type = output.get('output_type')

            # Handle execute_result and display_data outputs
            if output_type in ('execute_result', 'display_data'):
                data = output.get('data', {})

                # HTML (tables, formatted output)
                if 'text/html' in data:
                    html_content = data['text/html']
                    if isinstance(html_content, list):
                        html_content = ''.join(html_content)
                    outputs_data['html'].append(html_content)

                # Markdown
                if 'text/markdown' in data:
                    md_content = data['text/markdown']
                    if isinstance(md_content, list):
                        md_content = ''.join(md_content)
                    outputs_data['markdown'].append(md_content)

                # Plain text
                if 'text/plain' in data:
                    text_content = data['text/plain']
                    if isinstance(text_content, list):
                        text_content = ''.join(text_content)
                    outputs_data['text'].append(text_content)

                # LaTeX
                if 'text/latex' in data:
                    latex_content = data['text/latex']
                    if isinstance(latex_content, list):
                        latex_content = ''.join(latex_content)
                    outputs_data['latex'].append(latex_content)

                # Images (base64 encoded - not used here, images handled separately)
                if 'image/png' in data:
                    outputs_data['images'].append(data['image/png'])

            # Handle stream outputs (print statements)
            elif output_type == 'stream':
                stream_text = output.get('text', [])
                if isinstance(stream_text, list):
                    stream_text = ''.join(stream_text)
                outputs_data['text'].append(stream_text)

        # If we found the specific cell, stop searching
        if cell_id and outputs_data != {'text': [], 'html': [], 'markdown': [], 'latex': [], 'images': []}:
            break

    # Remove empty lists
    outputs_data = {k: v for k, v in outputs_data.items() if v}

    # If looking for a specific cell but found nothing, try extracting ALL outputs
    if cell_id and not outputs_data:
        print(f"      WARNING: Cell '{cell_id}' not found, extracting all outputs instead", file=sys.stderr)
        return _extract_cell_outputs_from_notebook(notebook_path, cell_id=None)

    return outputs_data

def main():
    try:
        with open('.github/config.yml', 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
    except FileNotFoundError:
        print("ERROR: .github/config.yml not found. Using default 'index.qmd'.", file=sys.stderr)
        config = {}
    
    report_file = config.get('report_file', 'index.qmd')

    print(f"Parsing report file: {report_file}...")
    parsed = parse_quarto(report_file)

    try:
        with open('parsed_report.json', 'w', encoding='utf-8') as f:
            json.dump(parsed, f, indent=2, ensure_ascii=False)
    except Exception as e:
        print(f"ERROR: Failed to save parsed report to 'parsed_report.json': {e}", file=sys.stderr)
        sys.exit(1)

    print("\n✅ Report parsed successfully:")
    stats = parsed['stats']
    print(f"   - {stats['word_count']} words")
    print(f"   - {stats['figures']} figures (manual + generated)")

    manual_figs = sum(1 for f in parsed['figures']['details'] if f['source'].startswith('markdown:'))
    mapped_figs = sum(1 for f in parsed['figures']['details'] if not f['source'].startswith('markdown:'))
    print(f"     - {manual_figs} manually linked")
    print(f"     - {mapped_figs} generated from notebooks")

    # Print notebook output statistics
    nb_outputs = parsed.get('notebook_outputs', [])
    if nb_outputs:
        print(f"   - {len(nb_outputs)} notebook cell(s) with outputs extracted")
        total_tables = sum(len(nb['outputs'].get('html_as_markdown', [])) for nb in nb_outputs)
        total_text = sum(len(nb['outputs'].get('text', [])) for nb in nb_outputs)
        if total_tables > 0:
            print(f"     - {total_tables} HTML table(s) converted to markdown")
        if total_text > 0:
            print(f"     - {total_text} text output(s)")

if __name__ == '__main__':
    main()
