# AI Feedback System - Session Summary

## What We Built Today

### üéØ Major Achievement
Transformed the AI feedback system from a single-course, monolithic design to a **flexible, template-based system** that works for any course or assignment type.

---

## System Architecture

### Per-Assignment Self-Contained Design

Each assignment repo now contains everything it needs:

```
assignment-repo/
  .github/
    feedback/
      config.yml      # Assignment settings (name, course, model)
      rubric.yml      # Custom rubric for THIS assignment
      guidance.md     # AI instructions for THIS assignment

      templates/      # Starting points
        lab-rubric-template.yml
        programming-assignment-rubric-template.yml
        guidance-template.md

      examples/       # Course-specific examples
        eeng-320-lab-example.yml
        phys-280-assignment-example.yml
        phys-230-lab-example.yml
        eeng-340-project-example.yml

    workflows/
      report-feedback.yml   # GitHub Actions workflow

 /
    parse_report.py
    section_extractor.py
    ai_feedback_criterion.py   # ‚Üê Updated for new format!
    create_issue.py
```

---

## Key Improvements

### 1. Simplified Rubric Format ‚úÖ

**Old (EENG-320 specific)**:
```yaml
prelab:
  criteria: [...]
lab_report:
  criteria: [...]
final_project:
  criteria: [...]
```

**New (universal)**:
```yaml
assignment:
  name: "Assignment Name"
  course: "Course Code"
  total_points: 100

criteria:
  - id: criterion_1
    name: "..."
    weight: 20
    # ...
```

### 2. Template System ‚úÖ

Three ready-to-use templates:
- **Lab rubric** - For experimental/hands-on work
- **Programming rubric** - For coding assignments
- **Guidance template** - Instructions for AI

### 3. Course-Specific Examples ‚úÖ

Four complete examples:
- **EENG-320** (Electronics) - Circuit design, simulation, experimentation
- **PHYS-280** (Scientific Computing) - Algorithms, code quality, performance
- **PHYS-230** (Lab Instrumentation) - Calibration, measurements, uncertainty
- **EENG-340** (Interfacing Lab) - Hardware, firmware, testing

### 4. Updated Scripts ‚úÖ

- `ai_feedback_criterion.py` - Now handles simplified rubric format
- Cleaner output structure
- Enterprise token limit awareness

---

## Files Created/Updated

### Documentation üìö
- `INSTRUCTOR_GUIDE.md` - Complete guide for instructors
- `TEMPLATE_SYSTEM.md` - Template system overview
- `TEST_DEPLOYMENT.md` - Testing instructions
- `SUMMARY.md` - This file

### Templates üìã
- `templates/lab-rubric-template.yml`
- `templates/programming-assignment-rubric-template.yml`
- `templates/guidance-template.md`
- `config-template.yml`

### Examples üìù
- `examples/eeng-320-lab-example.yml`
- `examples/phys-280-assignment-example.yml`
- `examples/phys-230-lab-example.yml`
- `examples/eeng-340-project-example.yml`

### Updated Scripts üíª
- `scripts/ai_feedback_criterion.py` - Simplified rubric support

---

## Current Status

### ‚úÖ Completed
- [x] Template-based architecture designed
- [x] Simplified rubric format implemented
- [x] Scripts updated for new format
- [x] 4 course-specific examples created
- [x] Comprehensive documentation written
- [x] Test environment prepared (EENG-340 curve tracer)

### ‚è≥ Ready for Testing
- [ ] Deploy to real GitHub repo
- [ ] Test with GitHub Actions (enterprise token limits)
- [ ] Review feedback quality
- [ ] Iterate on rubric/guidance based on results

### üöÄ Next Steps
1. Test with EENG-340 curve tracer project (ready to go!)
2. Test with PHYS-280, PHYS-230 assignments
3. Deploy to GitHub Classroom templates
4. Gather student feedback on AI feedback quality

---

## How to Use This System

### Quick Start

1. **Copy to your assignment repo**:
   ```bash
   cp -r ai-feedback-system/dot_github_folder your-assignment-repo/.github
   
   ```

2. **Customize rubric**:
   ```bash
   cd your-assignment-repo/.github/feedback
   cp examples/[your-course]-example.yml rubric.yml
   # Edit rubric.yml for this assignment
   ```

3. **Customize guidance**:
   ```bash
   cp templates/guidance-template.md guidance.md
   # Fill in assignment-specific details
   ```

4. **Deploy and test**:
   ```bash
   git add .github
   git commit -m "Add AI feedback system"
   git push
   git tag feedback-test
   git push origin feedback-test
   ```

---

## Enterprise Token Limits

### Discovery
- ‚úÖ Enterprise/Education accounts have **higher limits** than personal accounts
- ‚úÖ ~5,000 requests/hour (vs 15/min personal)
- ‚ö†Ô∏è Still limited to ~2 concurrent requests
- ‚úÖ Higher per-request token limits (need to verify in GitHub Actions)

### Impact on Design
- **Keep criterion-based approach** - Still optimal for quality
- **Concurrency = 2** - Respects concurrent request limits
- **Parallel processing** - 6 criteria in ~3 batches = ~10-15 seconds
- **Higher limits = headroom** - Less worry about edge cases

---

## Testing Plan

### Phase 1: Single Course Test
**Test**: EENG-340 Curve Tracer Project
**Status**: Config ready, waiting for GitHub Actions deployment
**Expected**: 6 API calls, ~15-20K feedback characters

### Phase 2: Multi-Course Test
- PHYS-280: Programming assignment
- PHYS-230: Instrumentation lab
- Compare feedback quality across disciplines

### Phase 3: Student Feedback
- Deploy to real students next semester
- Collect feedback on AI feedback quality
- Iterate on rubrics and guidance

---

## Key Decisions Made

### 1. Per-Assignment vs. Course-Wide Rubrics
**Decision**: Per-assignment
**Reason**: Each assignment emphasizes different skills, needs custom rubrics

### 2. Simplified vs. Legacy Format
**Decision**: Simplified only (no backward compatibility)
**Reason**: Cleaner, easier to maintain. Will recreate prelabs as separate assignments.

### 3. Template-Based vs. Central Config
**Decision**: Template-based
**Reason**: More shareable, easier for other instructors to adopt

### 4. Keep Criterion-Based Analysis
**Decision**: Yes, even with higher token limits
**Reason**: Better quality, focused analysis, resilient to failures

---

## Metrics & Success Criteria

### Technical Success
- ‚úÖ Workflow runs without errors
- ‚úÖ All criteria analyzed (6/6 for EENG-340)
- ‚úÖ Feedback generated and posted as issue
- ‚úÖ Under token limits

### Quality Success
- ‚è≥ Feedback is specific (references sections, figures)
- ‚è≥ Actionable suggestions (concrete next steps)
- ‚è≥ Appropriate tone (encouraging, constructive)
- ‚è≥ Catches real issues students had

### Adoption Success
- ‚è≥ Easy for you to customize per assignment
- ‚è≥ Easy for other instructors to adapt
- ‚è≥ Students find feedback helpful

---

## Files Ready for Deployment

### Essential Files (copy these)
```
.github/
  feedback/
    config.yml
    rubric.yml
    guidance.md
  workflows/
    report-feedback.yml

scripts/
  parse_report.py
  section_extractor.py
  ai_feedback_criterion.py
  create_issue.py

```

### Reference Files (use as starting points)
```
.github/feedback/
  templates/
    lab-rubric-template.yml
    programming-assignment-rubric-template.yml
    guidance-template.md
  examples/
    eeng-320-lab-example.yml
    phys-280-assignment-example.yml
    phys-230-lab-example.yml
    eeng-340-project-example.yml
```

---

## What Makes This System Powerful

1. **Template-based** - Don't start from scratch
2. **Course-agnostic** - Works for any discipline
3. **Assignment-specific** - Each gets custom feedback
4. **Criterion-focused** - Detailed, structured feedback
5. **Shareable** - Other instructors can easily adopt
6. **Free** - GitHub education account (no cost)
7. **Automatic** - Students tag, feedback appears
8. **Quality** - Specific, actionable, encouraging

---

## Next Session Goals

1. **Deploy to GitHub repo** - Test with real GitHub Actions
2. **Verify enterprise limits** - Confirm token limits in practice
3. **Review feedback quality** - Is it helpful for this student?
4. **Test other courses** - PHYS-280, PHYS-230
5. **Iterate** - Refine rubrics based on results

---

## Questions for Testing

### Technical
- Do all 6 criteria analyze successfully?
- What's actual token usage per criterion?
- How long does complete analysis take?
- Does issue creation work correctly?

### Quality
- Is feedback specific to this student's work?
- Does it catch the issues we'd catch manually?
- Is tone appropriate and encouraging?
- Are suggestions actionable?

### Usability
- Is rubric easy to customize?
- Is guidance clear for AI?
- Would other instructors understand how to use this?

---

**Status**: üéâ Ready for deployment and testing!

**Next Step**: Copy to a real GitHub repo and test with GitHub Actions.

**Location**: `/Users/steve/Development/courses/eeng/eeng320/202510/repos/ai-feedback-system/`

**Test Command** (see TEST_DEPLOYMENT.md):
```bash
cd [test-repo]
cp -r [ai-feedback-system]/dot_github_folder .github
cp -r [ai-feedback-system]/scripts .
git add .github
git commit -m "Test AI feedback system"
git push
git tag feedback-test
git push origin feedback-test
# Watch in Actions tab!
```
